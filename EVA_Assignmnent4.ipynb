{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_Assignmnent4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_IT9nB-RFdz",
        "colab_type": "code",
        "outputId": "818f46a1-a243-42fe-9076-9af49a153ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuZvocQiVdsc",
        "colab_type": "text"
      },
      "source": [
        "**Importing the numpy, keras libraries for mathematical computational and building convolutional neural network respectively**\n",
        "\n",
        "**Importing MNIST dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ScVBlkRSCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZp6teuYRV6_",
        "colab_type": "text"
      },
      "source": [
        "**Load pre-shuffled MNIST data into train and test sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5VINCpARYaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miNCQz1kV447",
        "colab_type": "text"
      },
      "source": [
        "**Printing the shape of the train dataset. It says that there are 60000 images of dimension 28X28X3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWU8_o-aRfiR",
        "colab_type": "code",
        "outputId": "880de895-afa2-4a54-d586-d559fedba8bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f077d281c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa5aaLaUWIvX",
        "colab_type": "text"
      },
      "source": [
        "**Reshaping the RGB image to gray scale**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjhCQQKeRlHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFCx5ZLIWT2D",
        "colab_type": "text"
      },
      "source": [
        "**Normalizing the grey scale image, where 0 represents black and 255 represents white**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_34Tx6NQRtib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt4doaF2WhGP",
        "colab_type": "text"
      },
      "source": [
        "**Printing the dependent varaible, which we are trying to predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfFCcSQ8RxTA",
        "colab_type": "code",
        "outputId": "007a9075-7f96-40f3-9379-35eff3b089d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qyhz026WvJ5",
        "colab_type": "text"
      },
      "source": [
        "**Convert 1-dimensional class arrays to 10-dimensional class matrices**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-lY5bCMR4Eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O_RALQPW2tT",
        "colab_type": "text"
      },
      "source": [
        "**Printing the dependent after converting it into 10-dimensional**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3-vfcV6R_mj",
        "colab_type": "code",
        "outputId": "21c3914c-13f4-488b-98b2-33d4e48bbb1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFELDUCnWzVw",
        "colab_type": "text"
      },
      "source": [
        "**Building Convolutional network such that the parameters used are less than 15000. The global receptive field is maintained as the size of the image.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9WsBxSlxY1l",
        "colab_type": "text"
      },
      "source": [
        "# ***Convolution Neural Network 1: Vanilla Network***\n",
        "\n",
        "**Network architecture is the same as that in assignment 3, except that the number of kernels used in this network are less.** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-o7LlE7SA44",
        "colab_type": "code",
        "outputId": "ee149571-1cbc-4a8a-a7aa-7bacb2066c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        }
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "model = Sequential() \n",
        "model.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1))) # I/P=28X28X1 | (3X3X1)X10 | O/P 26X26X10 | RF=3X3\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu'))                        # I/P= 26X26X10 | (3X3X18)X20 | O/P 24X24X20 | RF=5X5\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))                                    # I/P= 24X24X20 | MP (2X2) | O/P 12X12X20 | RF=10X10\n",
        "model.add(Convolution2D(20,3,3, activation='relu'))                          # I/P= 12X12X20 | (3X3X20)X20 | O/P 10X10X20 | RF=12X12\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))                                    # I/P= 10X10X20 | MP (2X2) | O/P 5X5X20 | RF=24X24\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu'))                        # I/P= 5X5X20 | MP (3X3X22)X20 | O/P 3X3X20 | RF=26X26\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu'))                        # I/P= 3X3X20 | (1X1X20)X10 | O/P 3X3X10 | RF=26X26\n",
        "model.add(Convolution2D(10, 3, 3))                                           # I/P= 3X3X10 |  (3X3X10)X10 | O/P 1X3X10 | RF=28X28\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 20)        1820      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 10, 10, 20)        3620      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 3, 3, 20)          3620      \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 3, 3, 10)          210       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 1, 1, 10)          910       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 10,280\n",
            "Trainable params: 10,280\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3))`\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIFg6MhySHxp",
        "colab_type": "code",
        "outputId": "2a4638df-6b09-4283-bc7f-eea9e9244cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG1m9Aa2bkQ0",
        "colab_type": "text"
      },
      "source": [
        "**Training the model by setting the batch size and number of epochs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyVkUMtgSMem",
        "colab_type": "code",
        "outputId": "ee28533f-3999-45be-fa63-cfd1448e0205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        }
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, epochs=20, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 20s 330us/step - loss: 0.2352 - acc: 0.9262 - val_loss: 0.0728 - val_acc: 0.9758\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.0796 - acc: 0.9759 - val_loss: 0.0503 - val_acc: 0.9834\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0604 - acc: 0.9819 - val_loss: 0.0490 - val_acc: 0.9845\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.0492 - acc: 0.9849 - val_loss: 0.0466 - val_acc: 0.9854\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.0427 - acc: 0.9864 - val_loss: 0.0458 - val_acc: 0.9844\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.0356 - acc: 0.9888 - val_loss: 0.0493 - val_acc: 0.9843\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.0329 - acc: 0.9889 - val_loss: 0.0439 - val_acc: 0.9865\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.0286 - acc: 0.9906 - val_loss: 0.0417 - val_acc: 0.9865\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.0250 - acc: 0.9921 - val_loss: 0.0418 - val_acc: 0.9878\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.0229 - acc: 0.9927 - val_loss: 0.0340 - val_acc: 0.9895\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0209 - acc: 0.9933 - val_loss: 0.0355 - val_acc: 0.9903\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.0196 - acc: 0.9932 - val_loss: 0.0412 - val_acc: 0.9896\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0172 - acc: 0.9945 - val_loss: 0.0424 - val_acc: 0.9884\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.0160 - acc: 0.9943 - val_loss: 0.0576 - val_acc: 0.9872\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.0163 - acc: 0.9945 - val_loss: 0.0366 - val_acc: 0.9899\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.0147 - acc: 0.9949 - val_loss: 0.0403 - val_acc: 0.9885\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0125 - acc: 0.9958 - val_loss: 0.0385 - val_acc: 0.9892\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.0126 - acc: 0.9956 - val_loss: 0.0454 - val_acc: 0.9887\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.0115 - acc: 0.9960 - val_loss: 0.0440 - val_acc: 0.9903\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.0102 - acc: 0.9961 - val_loss: 0.0555 - val_acc: 0.9869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f077a0b5f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjHsb3FZbwj9",
        "colab_type": "text"
      },
      "source": [
        "**Evaluating the trained model on the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCaOek5vZVs5",
        "colab_type": "code",
        "outputId": "4e284e07-8c93-49ea-b798-ea7d64f0faa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.055512672307605954, 0.9869]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNL0XTdLyAds",
        "colab_type": "text"
      },
      "source": [
        "**ACCURACY OF NETWORK 1: 99.03 (11th Epoch)**\n",
        "\n",
        "****\n",
        "**RESULT OF NETWORK 1:**\n",
        "\n",
        "For network 1,  we can observe that the training accuracy is more than the validation accuracy indicating model overfitting. \n",
        "*****\n",
        "**NEXT STEP:**\n",
        "\n",
        "Therefore, as a next step of improvement Batch Normalization and drop out are added to the network. Drop outhelps in reducing the model overfitting by adding noise at each layer. By adding dropout, we lose some information. But when we use batch normalization(which helps in reducing covariance shift) less dropout can be used to avoid losing lot of information \n",
        "*******\n",
        "**NETWORK 2: ADDING BATCH NORMALIZATION AND DROP OUT**\n",
        "\n",
        "As a second step, batch normalization and drop out are added after each convolution. Drop out of 0.1 is added at the end of each convolution. This means that a particular neuron is retained with probability of 0.1 after the training.\n",
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMVXNq4MjUjL",
        "colab_type": "text"
      },
      "source": [
        "# **CNN 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZlGS9WUn9Gs",
        "colab_type": "code",
        "outputId": "b1177d73-538f-4410-d6a9-ed9ffcbdb884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D\n",
        "model = Sequential() \n",
        "model.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1))) # I/P=28X28X1   | (3X3X1)X10  | O/P 26X26X10 | RF=3X3\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu'))                        # I/P= 26X26X10 | (3X3X18)X20 | O/P 24X24X20 | RF=5X5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))                                    # I/P= 24X24X20 | MP (2X2)    | O/P 12X12X20 | RF=10X10\n",
        "model.add(Convolution2D(20,3,3, activation='relu'))                          # I/P= 12X12X20 | (3X3X20)X20 | O/P 10X10X20 | RF=12X12\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))                                    # I/P= 10X10X20 | MP (2X2)    | O/P 5X5X20   | RF=24X24\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu'))                        # I/P= 5X5X20   | (3X3X20)X20 | O/P 3X3X20   | RF=26X26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu'))                        # I/P= 3X3X20   | (1X1X20)X10 | O/P 3X3X10   | RF=26X26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 3, 3))                                           # I/P= 3X3X10   | (3X3X10)X10 | O/P 1X1X10   | RF=28X28\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, batch_size=32, epochs=20, verbose=1, validation_data=(X_test, Y_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 24, 24, 20)        1820      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 20)        80        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 20)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 10, 10, 20)        3620      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 10, 10, 20)        80        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 10, 10, 20)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 3, 3, 20)          3620      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 3, 3, 20)          80        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 3, 3, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 3, 3, 10)          210       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 3, 3, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 3, 3, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 1, 1, 10)          910       \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 10,600\n",
            "Trainable params: 10,440\n",
            "Non-trainable params: 160\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 32s 541us/step - loss: 0.2625 - acc: 0.9227 - val_loss: 0.0596 - val_acc: 0.9814\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 32s 525us/step - loss: 0.0802 - acc: 0.9751 - val_loss: 0.0429 - val_acc: 0.9852\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 31s 521us/step - loss: 0.0620 - acc: 0.9806 - val_loss: 0.0430 - val_acc: 0.9859\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 31s 524us/step - loss: 0.0536 - acc: 0.9834 - val_loss: 0.0314 - val_acc: 0.9892\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 31s 523us/step - loss: 0.0466 - acc: 0.9856 - val_loss: 0.0286 - val_acc: 0.9906\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0447 - acc: 0.9863 - val_loss: 0.0278 - val_acc: 0.9920\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 0.0388 - acc: 0.9880 - val_loss: 0.0281 - val_acc: 0.9903\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0364 - acc: 0.9884 - val_loss: 0.0302 - val_acc: 0.9905\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 30s 508us/step - loss: 0.0361 - acc: 0.9888 - val_loss: 0.0281 - val_acc: 0.9909\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 31s 517us/step - loss: 0.0343 - acc: 0.9890 - val_loss: 0.0227 - val_acc: 0.9927\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0323 - acc: 0.9899 - val_loss: 0.0245 - val_acc: 0.9925\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 31s 514us/step - loss: 0.0311 - acc: 0.9897 - val_loss: 0.0201 - val_acc: 0.9931\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0308 - acc: 0.9901 - val_loss: 0.0212 - val_acc: 0.9936\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 0.0286 - acc: 0.9910 - val_loss: 0.0256 - val_acc: 0.9919\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 32s 527us/step - loss: 0.0280 - acc: 0.9908 - val_loss: 0.0259 - val_acc: 0.9916\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 31s 514us/step - loss: 0.0265 - acc: 0.9917 - val_loss: 0.0255 - val_acc: 0.9916\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 31s 517us/step - loss: 0.0263 - acc: 0.9912 - val_loss: 0.0250 - val_acc: 0.9916\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 31s 521us/step - loss: 0.0238 - acc: 0.9923 - val_loss: 0.0262 - val_acc: 0.9915\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.0251 - acc: 0.9919 - val_loss: 0.0229 - val_acc: 0.9922\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 31s 521us/step - loss: 0.0240 - acc: 0.9917 - val_loss: 0.0237 - val_acc: 0.9928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f07706b62b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElAhJvnQFzl8",
        "colab_type": "code",
        "outputId": "dc2415d5-66b5-4455-ad09-641cc2156703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.023711118094769335, 0.9928]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn74J8TKl5l2",
        "colab_type": "text"
      },
      "source": [
        "**ACCURACY OF NETWORK 2: 99.36 (Epoch 13)**\n",
        "******\n",
        "**RESULT OF NETWORK 2:**\n",
        "\n",
        "We can see that overfitting is vanished now\n",
        "*******\n",
        "**NEURAL NETWORK 3:**\n",
        "Increasing the number of kernels and Dropout of 0.25 only after Maxpooling\n",
        "*************\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8SG9Eg8n2si",
        "colab_type": "text"
      },
      "source": [
        "# CNN3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_Y7NE3LF28y",
        "colab_type": "code",
        "outputId": "6c6df0ff-a877-4ecc-9054-2789ba39e88d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D\n",
        "model = Sequential() \n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1)))    # I/P=28X28X1    | (3X3X1)X16     | O/P 26X26X16 | RF=3X3\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu'))                           # I/P= 26X26X16  | (3X3X16)X20    | O/P 24X24X20 | RF=5X5\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))                                       # I/P= 24X24X20  | MP (2X2)       | O/P 12X12X20 | RF=10X10\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Convolution2D(22,3,3, activation='relu'))                             # I/P= 12X12X20  | (3X3X20)X22    | O/P 10X10X22 | RF=12X12\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))                                       # I/P= 10X10X22  | MP (2X2)       | O/P 5X5X22   | RF=24X24\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))                           # I/P= 5X5X22    | (3X3X22)X24    | O/P 3X3X24   | RF=26X26\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu'))                           # I/P= 3X3X24    | (1X1X24)X10    | O/P 3X3X10   | RF=26X26\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(10, 3, 3))                                              # I/P= 3X3X10    |  (3X3X10)X10   | O/P 1X1X10   | RF=28X28\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, batch_size=32, epochs=20, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(22, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 24, 24, 20)        2900      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 24, 24, 20)        80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 20)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 12, 12, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 10, 10, 22)        3982      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 10, 10, 22)        88        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 22)          0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 5, 5, 22)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 3, 3, 24)          4776      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 3, 3, 24)          96        \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 3, 3, 10)          250       \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 3, 3, 10)          40        \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 1, 1, 10)          910       \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 13,346\n",
            "Trainable params: 13,162\n",
            "Non-trainable params: 184\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 31s 517us/step - loss: 0.2594 - acc: 0.9223 - val_loss: 0.0551 - val_acc: 0.9839\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 30s 501us/step - loss: 0.0726 - acc: 0.9771 - val_loss: 0.0402 - val_acc: 0.9865\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 30s 497us/step - loss: 0.0585 - acc: 0.9817 - val_loss: 0.0367 - val_acc: 0.9875\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 30s 495us/step - loss: 0.0497 - acc: 0.9840 - val_loss: 0.0295 - val_acc: 0.9894\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 29s 491us/step - loss: 0.0446 - acc: 0.9856 - val_loss: 0.0310 - val_acc: 0.9896\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 30s 493us/step - loss: 0.0417 - acc: 0.9863 - val_loss: 0.0314 - val_acc: 0.9900\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 30s 493us/step - loss: 0.0387 - acc: 0.9872 - val_loss: 0.0285 - val_acc: 0.9911\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 0.0364 - acc: 0.9885 - val_loss: 0.0262 - val_acc: 0.9916\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 30s 495us/step - loss: 0.0338 - acc: 0.9888 - val_loss: 0.0243 - val_acc: 0.9917\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 30s 496us/step - loss: 0.0321 - acc: 0.9896 - val_loss: 0.0264 - val_acc: 0.9906\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 29s 490us/step - loss: 0.0308 - acc: 0.9902 - val_loss: 0.0268 - val_acc: 0.9912\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 30s 504us/step - loss: 0.0291 - acc: 0.9902 - val_loss: 0.0277 - val_acc: 0.9919\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 29s 485us/step - loss: 0.0293 - acc: 0.9904 - val_loss: 0.0278 - val_acc: 0.9913\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 30s 492us/step - loss: 0.0263 - acc: 0.9918 - val_loss: 0.0252 - val_acc: 0.9921\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 30s 492us/step - loss: 0.0264 - acc: 0.9914 - val_loss: 0.0248 - val_acc: 0.9923\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 29s 491us/step - loss: 0.0251 - acc: 0.9919 - val_loss: 0.0269 - val_acc: 0.9911\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 29s 491us/step - loss: 0.0262 - acc: 0.9919 - val_loss: 0.0258 - val_acc: 0.9913\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 29s 490us/step - loss: 0.0245 - acc: 0.9920 - val_loss: 0.0255 - val_acc: 0.9928\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 29s 491us/step - loss: 0.0246 - acc: 0.9921 - val_loss: 0.0247 - val_acc: 0.9929\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 30s 496us/step - loss: 0.0233 - acc: 0.9922 - val_loss: 0.0257 - val_acc: 0.9930\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f072cfdc940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vttLDCNQIJMD",
        "colab_type": "code",
        "outputId": "adf328f2-888f-4d10-8c59-567667cd112e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.025668010003571544, 0.993]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hauHYuK_oBUZ",
        "colab_type": "text"
      },
      "source": [
        "**ACCURACY OF NETWORK 3: 99.3 (Epoch 20)**\n",
        "*****************\n",
        "**RESULT OF NETWORK 3:**\n",
        "\n",
        "The model accuracy has reduced from 99.36 to 99.3. We can also see that the gap between training accuracy and validation accuracy is less. This may be because of the increase in number of kernels.\n",
        "\n",
        "******************\n",
        "\n",
        "**NEURAL NETWORK 4:**\n",
        "\n",
        "As a next step, number of kernels are set equal to that in the network 2 and Learning rate has been changed.\n",
        "******************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlG1R0M1sUKz",
        "colab_type": "text"
      },
      "source": [
        "# CNN 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRoxGbtO3RTO",
        "colab_type": "code",
        "outputId": "cde96cdb-76e6-40d0-d6eb-40ecc7dbd112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D\n",
        "model = Sequential() \n",
        "model.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1)))    # I/P= 28X28X1   | (3X3X1)X10   | O/P 26X26X10 | RF=3X3\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu'))                           # I/P= 26X26X10  | (3X3X10)X20  | O/P 24X24X20 | RF=5X5\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))                                       # I/P= 24X264X20 | MP (2X2)     | O/P 12X12X20 | RF=10X10\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Convolution2D(20,3,3, activation='relu'))                             # I/P= 12X12X20  | (3X3X20)X20  | O/P 10X10X20 | RF=12X12\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))                                       # I/P= 10X10X20  | MP (2X2)     | O/P 5X5X20   | RF=24X24\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu'))                           # I/P= 5X5X20    | (3X3X20)X20  | O/P 3X3X20   | RF=26X26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu'))                           # I/P= 3X3X20    | (1X1X20)X10  | O/P 3X3X10   | RF=26X26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(10, 3, 3))                                              # I/P= 3X3X10    |  (3X3X10)X10 | O/P 1X1X10   | RF=28X28\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 24, 24, 20)        1820      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 24, 24, 20)        80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 20)        0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 12, 12, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 10, 10, 20)        3620      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 10, 10, 20)        80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 5, 5, 20)          0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 5, 5, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 3, 3, 20)          3620      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 3, 3, 20)          80        \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 3, 3, 10)          210       \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 3, 3, 10)          40        \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 1, 1, 10)          910       \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 10,600\n",
            "Trainable params: 10,440\n",
            "Non-trainable params: 160\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.2580 - acc: 0.9212 - val_loss: 0.0685 - val_acc: 0.9779\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 10s 160us/step - loss: 0.0738 - acc: 0.9764 - val_loss: 0.0356 - val_acc: 0.9866\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0545 - acc: 0.9824 - val_loss: 0.0313 - val_acc: 0.9892\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0471 - acc: 0.9848 - val_loss: 0.0259 - val_acc: 0.9906\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 10s 161us/step - loss: 0.0421 - acc: 0.9864 - val_loss: 0.0291 - val_acc: 0.9895\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 10s 161us/step - loss: 0.0371 - acc: 0.9882 - val_loss: 0.0250 - val_acc: 0.9910\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 10s 159us/step - loss: 0.0345 - acc: 0.9886 - val_loss: 0.0228 - val_acc: 0.9919\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 10s 160us/step - loss: 0.0330 - acc: 0.9890 - val_loss: 0.0233 - val_acc: 0.9914\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 10s 162us/step - loss: 0.0328 - acc: 0.9891 - val_loss: 0.0216 - val_acc: 0.9923\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0293 - acc: 0.9904 - val_loss: 0.0215 - val_acc: 0.9916\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0289 - acc: 0.9905 - val_loss: 0.0215 - val_acc: 0.9920\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0276 - acc: 0.9908 - val_loss: 0.0204 - val_acc: 0.9922\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0266 - acc: 0.9910 - val_loss: 0.0182 - val_acc: 0.9936\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0246 - acc: 0.9918 - val_loss: 0.0182 - val_acc: 0.9932\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 9s 153us/step - loss: 0.0268 - acc: 0.9910 - val_loss: 0.0179 - val_acc: 0.9936\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.0250 - acc: 0.9920 - val_loss: 0.0185 - val_acc: 0.9933\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.0224 - acc: 0.9927 - val_loss: 0.0178 - val_acc: 0.9932\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0219 - acc: 0.9934 - val_loss: 0.0181 - val_acc: 0.9934\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0224 - acc: 0.9930 - val_loss: 0.0164 - val_acc: 0.9940\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0230 - acc: 0.9925 - val_loss: 0.0169 - val_acc: 0.9940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f072bf197f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKd2TFmD39jO",
        "colab_type": "code",
        "outputId": "0613a101-15ea-4224-a8f9-f8e46f8f0815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=1)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 112us/step\n",
            "[0.01689683752369747, 0.994]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-pRDi7_sf8b",
        "colab_type": "text"
      },
      "source": [
        "# Accuracy of 99.4 is achieved with 10600 parameters."
      ]
    }
  ]
}